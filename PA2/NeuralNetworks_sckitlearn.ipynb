{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTrLJdrTcyRI"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "X_train, y_train = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train[50:],y_train[50:],test_size=0.2, random_state=1)\n",
        "\n",
        "for i in range(80):\n",
        "    if y_train[i]==1:\n",
        "        y_train[i]=-1\n",
        "    else:\n",
        "        y_train[i]=1\n",
        "for j in range(20):\n",
        "    if y_test[j]==1:\n",
        "        y_test[j]=-1\n",
        "    else:\n",
        "        y_test[j]=1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pct=MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(3, 3), random_state=1, max_iter=300, verbose=True)\n",
        "pct.fit(X_train,y_train)\n",
        "#Pass in the test features into the trained model\n",
        "pred_pct=pct.predict(X_test)\n",
        "print ('Test accuracy: ', pct.score(X_test, y_test))\n",
        "\n",
        "pred_pct=pct.predict(X_train)\n",
        "print ('Train accuracy: ',pct.score(X_train, y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QbO6xStdgpp",
        "outputId": "48b6d842-f28c-410e-b714-2fda0d2222b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.25806735\n",
            "Iteration 2, loss = 1.25600140\n",
            "Iteration 3, loss = 1.25396990\n",
            "Iteration 4, loss = 1.25195498\n",
            "Iteration 5, loss = 1.24998727\n",
            "Iteration 6, loss = 1.24804174\n",
            "Iteration 7, loss = 1.24612067\n",
            "Iteration 8, loss = 1.24420761\n",
            "Iteration 9, loss = 1.24231205\n",
            "Iteration 10, loss = 1.24043032\n",
            "Iteration 11, loss = 1.23856713\n",
            "Iteration 12, loss = 1.23672764\n",
            "Iteration 13, loss = 1.23489756\n",
            "Iteration 14, loss = 1.23308835\n",
            "Iteration 15, loss = 1.23129094\n",
            "Iteration 16, loss = 1.22950918\n",
            "Iteration 17, loss = 1.22773358\n",
            "Iteration 18, loss = 1.22596388\n",
            "Iteration 19, loss = 1.22419989\n",
            "Iteration 20, loss = 1.22244142\n",
            "Iteration 21, loss = 1.22068831\n",
            "Iteration 22, loss = 1.21894044\n",
            "Iteration 23, loss = 1.21720425\n",
            "Iteration 24, loss = 1.21547951\n",
            "Iteration 25, loss = 1.21375994\n",
            "Iteration 26, loss = 1.21204543\n",
            "Iteration 27, loss = 1.21034132\n",
            "Iteration 28, loss = 1.20864758\n",
            "Iteration 29, loss = 1.20695862\n",
            "Iteration 30, loss = 1.20527440\n",
            "Iteration 31, loss = 1.20359487\n",
            "Iteration 32, loss = 1.20192001\n",
            "Iteration 33, loss = 1.20024979\n",
            "Iteration 34, loss = 1.19858416\n",
            "Iteration 35, loss = 1.19692312\n",
            "Iteration 36, loss = 1.19526663\n",
            "Iteration 37, loss = 1.19361468\n",
            "Iteration 38, loss = 1.19196725\n",
            "Iteration 39, loss = 1.19032433\n",
            "Iteration 40, loss = 1.18868589\n",
            "Iteration 41, loss = 1.18705193\n",
            "Iteration 42, loss = 1.18542243\n",
            "Iteration 43, loss = 1.18379738\n",
            "Iteration 44, loss = 1.18217677\n",
            "Iteration 45, loss = 1.18056059\n",
            "Iteration 46, loss = 1.17894883\n",
            "Iteration 47, loss = 1.17734148\n",
            "Iteration 48, loss = 1.17573852\n",
            "Iteration 49, loss = 1.17413996\n",
            "Iteration 50, loss = 1.17254578\n",
            "Iteration 51, loss = 1.17095598\n",
            "Iteration 52, loss = 1.16937054\n",
            "Iteration 53, loss = 1.16778946\n",
            "Iteration 54, loss = 1.16621299\n",
            "Iteration 55, loss = 1.16464534\n",
            "Iteration 56, loss = 1.16308201\n",
            "Iteration 57, loss = 1.16152300\n",
            "Iteration 58, loss = 1.15996833\n",
            "Iteration 59, loss = 1.15841797\n",
            "Iteration 60, loss = 1.15687193\n",
            "Iteration 61, loss = 1.15533020\n",
            "Iteration 62, loss = 1.15379279\n",
            "Iteration 63, loss = 1.15225968\n",
            "Iteration 64, loss = 1.15073087\n",
            "Iteration 65, loss = 1.14920637\n",
            "Iteration 66, loss = 1.14768616\n",
            "Iteration 67, loss = 1.14617023\n",
            "Iteration 68, loss = 1.14465859\n",
            "Iteration 69, loss = 1.14315123\n",
            "Iteration 70, loss = 1.14164815\n",
            "Iteration 71, loss = 1.14014933\n",
            "Iteration 72, loss = 1.13865477\n",
            "Iteration 73, loss = 1.13716447\n",
            "Iteration 74, loss = 1.13567842\n",
            "Iteration 75, loss = 1.13419661\n",
            "Iteration 76, loss = 1.13271904\n",
            "Iteration 77, loss = 1.13124570\n",
            "Iteration 78, loss = 1.12977658\n",
            "Iteration 79, loss = 1.12831169\n",
            "Iteration 80, loss = 1.12685100\n",
            "Iteration 81, loss = 1.12539452\n",
            "Iteration 82, loss = 1.12394224\n",
            "Iteration 83, loss = 1.12249415\n",
            "Iteration 84, loss = 1.12105025\n",
            "Iteration 85, loss = 1.11961052\n",
            "Iteration 86, loss = 1.11817497\n",
            "Iteration 87, loss = 1.11674358\n",
            "Iteration 88, loss = 1.11531635\n",
            "Iteration 89, loss = 1.11389326\n",
            "Iteration 90, loss = 1.11247432\n",
            "Iteration 91, loss = 1.11105952\n",
            "Iteration 92, loss = 1.10964885\n",
            "Iteration 93, loss = 1.10824229\n",
            "Iteration 94, loss = 1.10683985\n",
            "Iteration 95, loss = 1.10544152\n",
            "Iteration 96, loss = 1.10404729\n",
            "Iteration 97, loss = 1.10265716\n",
            "Iteration 98, loss = 1.10127110\n",
            "Iteration 99, loss = 1.09988913\n",
            "Iteration 100, loss = 1.09851122\n",
            "Iteration 101, loss = 1.09713738\n",
            "Iteration 102, loss = 1.09576760\n",
            "Iteration 103, loss = 1.09440187\n",
            "Iteration 104, loss = 1.09304017\n",
            "Iteration 105, loss = 1.09168251\n",
            "Iteration 106, loss = 1.09032888\n",
            "Iteration 107, loss = 1.08897926\n",
            "Iteration 108, loss = 1.08763366\n",
            "Iteration 109, loss = 1.08629206\n",
            "Iteration 110, loss = 1.08495445\n",
            "Iteration 111, loss = 1.08362083\n",
            "Iteration 112, loss = 1.08229120\n",
            "Iteration 113, loss = 1.08096554\n",
            "Iteration 114, loss = 1.07964384\n",
            "Iteration 115, loss = 1.07832611\n",
            "Iteration 116, loss = 1.07701232\n",
            "Iteration 117, loss = 1.07570248\n",
            "Iteration 118, loss = 1.07439657\n",
            "Iteration 119, loss = 1.07309459\n",
            "Iteration 120, loss = 1.07179653\n",
            "Iteration 121, loss = 1.07050238\n",
            "Iteration 122, loss = 1.06921213\n",
            "Iteration 123, loss = 1.06792579\n",
            "Iteration 124, loss = 1.06664333\n",
            "Iteration 125, loss = 1.06536475\n",
            "Iteration 126, loss = 1.06409005\n",
            "Iteration 127, loss = 1.06281921\n",
            "Iteration 128, loss = 1.06155223\n",
            "Iteration 129, loss = 1.06028911\n",
            "Iteration 130, loss = 1.05902982\n",
            "Iteration 131, loss = 1.05777437\n",
            "Iteration 132, loss = 1.05652274\n",
            "Iteration 133, loss = 1.05527494\n",
            "Iteration 134, loss = 1.05403094\n",
            "Iteration 135, loss = 1.05279075\n",
            "Iteration 136, loss = 1.05155436\n",
            "Iteration 137, loss = 1.05032175\n",
            "Iteration 138, loss = 1.04909292\n",
            "Iteration 139, loss = 1.04786786\n",
            "Iteration 140, loss = 1.04664657\n",
            "Iteration 141, loss = 1.04542903\n",
            "Iteration 142, loss = 1.04421524\n",
            "Iteration 143, loss = 1.04300520\n",
            "Iteration 144, loss = 1.04179888\n",
            "Iteration 145, loss = 1.04059629\n",
            "Iteration 146, loss = 1.03939741\n",
            "Iteration 147, loss = 1.03820224\n",
            "Iteration 148, loss = 1.03701078\n",
            "Iteration 149, loss = 1.03582300\n",
            "Iteration 150, loss = 1.03463891\n",
            "Iteration 151, loss = 1.03345850\n",
            "Iteration 152, loss = 1.03228175\n",
            "Iteration 153, loss = 1.03110867\n",
            "Iteration 154, loss = 1.02993923\n",
            "Iteration 155, loss = 1.02877344\n",
            "Iteration 156, loss = 1.02761129\n",
            "Iteration 157, loss = 1.02645277\n",
            "Iteration 158, loss = 1.02529786\n",
            "Iteration 159, loss = 1.02414657\n",
            "Iteration 160, loss = 1.02299888\n",
            "Iteration 161, loss = 1.02185479\n",
            "Iteration 162, loss = 1.02071428\n",
            "Iteration 163, loss = 1.01957736\n",
            "Iteration 164, loss = 1.01844401\n",
            "Iteration 165, loss = 1.01731422\n",
            "Iteration 166, loss = 1.01618798\n",
            "Iteration 167, loss = 1.01506530\n",
            "Iteration 168, loss = 1.01394615\n",
            "Iteration 169, loss = 1.01283054\n",
            "Iteration 170, loss = 1.01171844\n",
            "Iteration 171, loss = 1.01060987\n",
            "Iteration 172, loss = 1.00950480\n",
            "Iteration 173, loss = 1.00840322\n",
            "Iteration 174, loss = 1.00730514\n",
            "Iteration 175, loss = 1.00621054\n",
            "Iteration 176, loss = 1.00511942\n",
            "Iteration 177, loss = 1.00403176\n",
            "Iteration 178, loss = 1.00294756\n",
            "Iteration 179, loss = 1.00186681\n",
            "Iteration 180, loss = 1.00078950\n",
            "Iteration 181, loss = 0.99971563\n",
            "Iteration 182, loss = 0.99864518\n",
            "Iteration 183, loss = 0.99757814\n",
            "Iteration 184, loss = 0.99651452\n",
            "Iteration 185, loss = 0.99545430\n",
            "Iteration 186, loss = 0.99439747\n",
            "Iteration 187, loss = 0.99334402\n",
            "Iteration 188, loss = 0.99229395\n",
            "Iteration 189, loss = 0.99124725\n",
            "Iteration 190, loss = 0.99020391\n",
            "Iteration 191, loss = 0.98916392\n",
            "Iteration 192, loss = 0.98812728\n",
            "Iteration 193, loss = 0.98709397\n",
            "Iteration 194, loss = 0.98606399\n",
            "Iteration 195, loss = 0.98503732\n",
            "Iteration 196, loss = 0.98401397\n",
            "Iteration 197, loss = 0.98299392\n",
            "Iteration 198, loss = 0.98197717\n",
            "Iteration 199, loss = 0.98096370\n",
            "Iteration 200, loss = 0.97995351\n",
            "Iteration 201, loss = 0.97894658\n",
            "Iteration 202, loss = 0.97794293\n",
            "Iteration 203, loss = 0.97694252\n",
            "Iteration 204, loss = 0.97594536\n",
            "Iteration 205, loss = 0.97495144\n",
            "Iteration 206, loss = 0.97396074\n",
            "Iteration 207, loss = 0.97297327\n",
            "Iteration 208, loss = 0.97198901\n",
            "Iteration 209, loss = 0.97100796\n",
            "Iteration 210, loss = 0.97003010\n",
            "Iteration 211, loss = 0.96905543\n",
            "Iteration 212, loss = 0.96808394\n",
            "Iteration 213, loss = 0.96711562\n",
            "Iteration 214, loss = 0.96615046\n",
            "Iteration 215, loss = 0.96518846\n",
            "Iteration 216, loss = 0.96422961\n",
            "Iteration 217, loss = 0.96327389\n",
            "Iteration 218, loss = 0.96232131\n",
            "Iteration 219, loss = 0.96137185\n",
            "Iteration 220, loss = 0.96042551\n",
            "Iteration 221, loss = 0.95948227\n",
            "Iteration 222, loss = 0.95854213\n",
            "Iteration 223, loss = 0.95760508\n",
            "Iteration 224, loss = 0.95667111\n",
            "Iteration 225, loss = 0.95574021\n",
            "Iteration 226, loss = 0.95481239\n",
            "Iteration 227, loss = 0.95388762\n",
            "Iteration 228, loss = 0.95296589\n",
            "Iteration 229, loss = 0.95204722\n",
            "Iteration 230, loss = 0.95113157\n",
            "Iteration 231, loss = 0.95021895\n",
            "Iteration 232, loss = 0.94930934\n",
            "Iteration 233, loss = 0.94840275\n",
            "Iteration 234, loss = 0.94749915\n",
            "Iteration 235, loss = 0.94659855\n",
            "Iteration 236, loss = 0.94570094\n",
            "Iteration 237, loss = 0.94480630\n",
            "Iteration 238, loss = 0.94391462\n",
            "Iteration 239, loss = 0.94302591\n",
            "Iteration 240, loss = 0.94214015\n",
            "Iteration 241, loss = 0.94125734\n",
            "Iteration 242, loss = 0.94037746\n",
            "Iteration 243, loss = 0.93950051\n",
            "Iteration 244, loss = 0.93862648\n",
            "Iteration 245, loss = 0.93775537\n",
            "Iteration 246, loss = 0.93688715\n",
            "Iteration 247, loss = 0.93602183\n",
            "Iteration 248, loss = 0.93515941\n",
            "Iteration 249, loss = 0.93429986\n",
            "Iteration 250, loss = 0.93344318\n",
            "Iteration 251, loss = 0.93258937\n",
            "Iteration 252, loss = 0.93173841\n",
            "Iteration 253, loss = 0.93089030\n",
            "Iteration 254, loss = 0.93004503\n",
            "Iteration 255, loss = 0.92920259\n",
            "Iteration 256, loss = 0.92836298\n",
            "Iteration 257, loss = 0.92752619\n",
            "Iteration 258, loss = 0.92669220\n",
            "Iteration 259, loss = 0.92586101\n",
            "Iteration 260, loss = 0.92503262\n",
            "Iteration 261, loss = 0.92420701\n",
            "Iteration 262, loss = 0.92338417\n",
            "Iteration 263, loss = 0.92256411\n",
            "Iteration 264, loss = 0.92174680\n",
            "Iteration 265, loss = 0.92093225\n",
            "Iteration 266, loss = 0.92012044\n",
            "Iteration 267, loss = 0.91931138\n",
            "Iteration 268, loss = 0.91850504\n",
            "Iteration 269, loss = 0.91770142\n",
            "Iteration 270, loss = 0.91690051\n",
            "Iteration 271, loss = 0.91610231\n",
            "Iteration 272, loss = 0.91530681\n",
            "Iteration 273, loss = 0.91451400\n",
            "Iteration 274, loss = 0.91372387\n",
            "Iteration 275, loss = 0.91293642\n",
            "Iteration 276, loss = 0.91215163\n",
            "Iteration 277, loss = 0.91136950\n",
            "Iteration 278, loss = 0.91059003\n",
            "Iteration 279, loss = 0.90981319\n",
            "Iteration 280, loss = 0.90903900\n",
            "Iteration 281, loss = 0.90826743\n",
            "Iteration 282, loss = 0.90749848\n",
            "Iteration 283, loss = 0.90673214\n",
            "Iteration 284, loss = 0.90596841\n",
            "Iteration 285, loss = 0.90520728\n",
            "Iteration 286, loss = 0.90444873\n",
            "Iteration 287, loss = 0.90369277\n",
            "Iteration 288, loss = 0.90293938\n",
            "Iteration 289, loss = 0.90218856\n",
            "Iteration 290, loss = 0.90144030\n",
            "Iteration 291, loss = 0.90069458\n",
            "Iteration 292, loss = 0.89995142\n",
            "Iteration 293, loss = 0.89921078\n",
            "Iteration 294, loss = 0.89847268\n",
            "Iteration 295, loss = 0.89773710\n",
            "Iteration 296, loss = 0.89700403\n",
            "Iteration 297, loss = 0.89627346\n",
            "Iteration 298, loss = 0.89554540\n",
            "Iteration 299, loss = 0.89481982\n",
            "Iteration 300, loss = 0.89409673\n",
            "Test accuracy:  0.4\n",
            "Train accuracy:  0.525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pct=MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(2000, 2000), random_state=1, max_iter=300, verbose=True)\n",
        "pct.fit(X_train,y_train)\n",
        "#Pass in the test features into the trained model\n",
        "pred_pct=pct.predict(X_test)\n",
        "print ('Test accuracy: ', pct.score(X_test, y_test))\n",
        "\n",
        "pred_pct=pct.predict(X_train)\n",
        "print ('Train accuracy: ',pct.score(X_train, y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnUs68uSdhVg",
        "outputId": "505c9de8-b51a-4806-f4c8-3d7a526722af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.70158051\n",
            "Iteration 2, loss = 1.87568560\n",
            "Iteration 3, loss = 0.60836938\n",
            "Iteration 4, loss = 1.20024028\n",
            "Iteration 5, loss = 1.13552838\n",
            "Iteration 6, loss = 0.76925111\n",
            "Iteration 7, loss = 0.58067668\n",
            "Iteration 8, loss = 0.67705721\n",
            "Iteration 9, loss = 0.76808326\n",
            "Iteration 10, loss = 0.72235260\n",
            "Iteration 11, loss = 0.60427198\n",
            "Iteration 12, loss = 0.52360838\n",
            "Iteration 13, loss = 0.53139747\n",
            "Iteration 14, loss = 0.57158588\n",
            "Iteration 15, loss = 0.57935649\n",
            "Iteration 16, loss = 0.53793203\n",
            "Iteration 17, loss = 0.47977541\n",
            "Iteration 18, loss = 0.44748658\n",
            "Iteration 19, loss = 0.45616322\n",
            "Iteration 20, loss = 0.47220672\n",
            "Iteration 21, loss = 0.45959279\n",
            "Iteration 22, loss = 0.42189605\n",
            "Iteration 23, loss = 0.39148837\n",
            "Iteration 24, loss = 0.38607988\n",
            "Iteration 25, loss = 0.39208770\n",
            "Iteration 26, loss = 0.38429398\n",
            "Iteration 27, loss = 0.35884739\n",
            "Iteration 28, loss = 0.33421869\n",
            "Iteration 29, loss = 0.32788271\n",
            "Iteration 30, loss = 0.32942778\n",
            "Iteration 31, loss = 0.31738212\n",
            "Iteration 32, loss = 0.29483514\n",
            "Iteration 33, loss = 0.28122292\n",
            "Iteration 34, loss = 0.27965003\n",
            "Iteration 35, loss = 0.27383742\n",
            "Iteration 36, loss = 0.25706315\n",
            "Iteration 37, loss = 0.24288213\n",
            "Iteration 38, loss = 0.23916091\n",
            "Iteration 39, loss = 0.23449842\n",
            "Iteration 40, loss = 0.22115332\n",
            "Iteration 41, loss = 0.20965260\n",
            "Iteration 42, loss = 0.20628262\n",
            "Iteration 43, loss = 0.20103969\n",
            "Iteration 44, loss = 0.18998678\n",
            "Iteration 45, loss = 0.18227846\n",
            "Iteration 46, loss = 0.17964089\n",
            "Iteration 47, loss = 0.17320187\n",
            "Iteration 48, loss = 0.16452328\n",
            "Iteration 49, loss = 0.16062165\n",
            "Iteration 50, loss = 0.15729702\n",
            "Iteration 51, loss = 0.15038264\n",
            "Iteration 52, loss = 0.14509456\n",
            "Iteration 53, loss = 0.14263714\n",
            "Iteration 54, loss = 0.13774830\n",
            "Iteration 55, loss = 0.13267398\n",
            "Iteration 56, loss = 0.13024873\n",
            "Iteration 57, loss = 0.12671746\n",
            "Iteration 58, loss = 0.12232067\n",
            "Iteration 59, loss = 0.11999336\n",
            "Iteration 60, loss = 0.11711011\n",
            "Iteration 61, loss = 0.11343359\n",
            "Iteration 62, loss = 0.11135207\n",
            "Iteration 63, loss = 0.10887223\n",
            "Iteration 64, loss = 0.10581285\n",
            "Iteration 65, loss = 0.10399685\n",
            "Iteration 66, loss = 0.10177356\n",
            "Iteration 67, loss = 0.09926001\n",
            "Iteration 68, loss = 0.09770661\n",
            "Iteration 69, loss = 0.09565511\n",
            "Iteration 70, loss = 0.09363024\n",
            "Iteration 71, loss = 0.09223739\n",
            "Iteration 72, loss = 0.09031686\n",
            "Iteration 73, loss = 0.08871605\n",
            "Iteration 74, loss = 0.08735089\n",
            "Iteration 75, loss = 0.08563631\n",
            "Iteration 76, loss = 0.08440764\n",
            "Iteration 77, loss = 0.08299864\n",
            "Iteration 78, loss = 0.08159230\n",
            "Iteration 79, loss = 0.08048218\n",
            "Iteration 80, loss = 0.07912302\n",
            "Iteration 81, loss = 0.07805642\n",
            "Iteration 82, loss = 0.07689032\n",
            "Iteration 83, loss = 0.07578430\n",
            "Iteration 84, loss = 0.07479805\n",
            "Iteration 85, loss = 0.07369576\n",
            "Iteration 86, loss = 0.07280480\n",
            "Iteration 87, loss = 0.07177990\n",
            "Iteration 88, loss = 0.07092449\n",
            "Iteration 89, loss = 0.06999552\n",
            "Iteration 90, loss = 0.06915240\n",
            "Iteration 91, loss = 0.06830510\n",
            "Iteration 92, loss = 0.06749741\n",
            "Iteration 93, loss = 0.06670742\n",
            "Iteration 94, loss = 0.06594238\n",
            "Iteration 95, loss = 0.06519049\n",
            "Iteration 96, loss = 0.06446540\n",
            "Iteration 97, loss = 0.06374958\n",
            "Iteration 98, loss = 0.06306762\n",
            "Iteration 99, loss = 0.06238456\n",
            "Iteration 100, loss = 0.06173871\n",
            "Iteration 101, loss = 0.06108052\n",
            "Iteration 102, loss = 0.06047177\n",
            "Iteration 103, loss = 0.05984468\n",
            "Iteration 104, loss = 0.05925095\n",
            "Iteration 105, loss = 0.05866001\n",
            "Iteration 106, loss = 0.05807739\n",
            "Iteration 107, loss = 0.05752492\n",
            "Iteration 108, loss = 0.05695972\n",
            "Iteration 109, loss = 0.05641886\n",
            "Iteration 110, loss = 0.05589048\n",
            "Iteration 111, loss = 0.05535972\n",
            "Iteration 112, loss = 0.05484592\n",
            "Iteration 113, loss = 0.05434932\n",
            "Iteration 114, loss = 0.05384316\n",
            "Iteration 115, loss = 0.05335095\n",
            "Iteration 116, loss = 0.05287394\n",
            "Iteration 117, loss = 0.05240003\n",
            "Iteration 118, loss = 0.05192483\n",
            "Iteration 119, loss = 0.05145584\n",
            "Iteration 120, loss = 0.05101012\n",
            "Iteration 121, loss = 0.05056015\n",
            "Iteration 122, loss = 0.05011721\n",
            "Iteration 123, loss = 0.04968863\n",
            "Iteration 124, loss = 0.04926525\n",
            "Iteration 125, loss = 0.04884020\n",
            "Iteration 126, loss = 0.04843076\n",
            "Iteration 127, loss = 0.04801893\n",
            "Iteration 128, loss = 0.04761685\n",
            "Iteration 129, loss = 0.04722851\n",
            "Iteration 130, loss = 0.04683767\n",
            "Iteration 131, loss = 0.04645761\n",
            "Iteration 132, loss = 0.04608993\n",
            "Iteration 133, loss = 0.04572397\n",
            "Iteration 134, loss = 0.04536078\n",
            "Iteration 135, loss = 0.04501043\n",
            "Iteration 136, loss = 0.04467809\n",
            "Iteration 137, loss = 0.04437495\n",
            "Iteration 138, loss = 0.04407654\n",
            "Iteration 139, loss = 0.04382967\n",
            "Iteration 140, loss = 0.04353366\n",
            "Iteration 141, loss = 0.04317692\n",
            "Iteration 142, loss = 0.04276962\n",
            "Iteration 143, loss = 0.04240087\n",
            "Iteration 144, loss = 0.04209419\n",
            "Iteration 145, loss = 0.04184400\n",
            "Iteration 146, loss = 0.04163851\n",
            "Iteration 147, loss = 0.04145481\n",
            "Iteration 148, loss = 0.04124265\n",
            "Iteration 149, loss = 0.04094977\n",
            "Iteration 150, loss = 0.04064280\n",
            "Iteration 151, loss = 0.04028941\n",
            "Iteration 152, loss = 0.03999165\n",
            "Iteration 153, loss = 0.03968780\n",
            "Iteration 154, loss = 0.03941120\n",
            "Iteration 155, loss = 0.03921552\n",
            "Iteration 156, loss = 0.03904304\n",
            "Iteration 157, loss = 0.03891845\n",
            "Iteration 158, loss = 0.03885123\n",
            "Iteration 159, loss = 0.03898723\n",
            "Iteration 160, loss = 0.03902835\n",
            "Iteration 161, loss = 0.03911536\n",
            "Iteration 162, loss = 0.03857505\n",
            "Iteration 163, loss = 0.03798297\n",
            "Iteration 164, loss = 0.03732697\n",
            "Iteration 165, loss = 0.03703639\n",
            "Iteration 166, loss = 0.03708111\n",
            "Iteration 167, loss = 0.03721187\n",
            "Iteration 168, loss = 0.03732748\n",
            "Iteration 169, loss = 0.03699089\n",
            "Iteration 170, loss = 0.03653804\n",
            "Iteration 171, loss = 0.03604620\n",
            "Iteration 172, loss = 0.03571889\n",
            "Iteration 173, loss = 0.03556473\n",
            "Iteration 174, loss = 0.03558547\n",
            "Iteration 175, loss = 0.03570081\n",
            "Iteration 176, loss = 0.03572075\n",
            "Iteration 177, loss = 0.03581659\n",
            "Iteration 178, loss = 0.03557094\n",
            "Iteration 179, loss = 0.03542297\n",
            "Iteration 180, loss = 0.03499157\n",
            "Iteration 181, loss = 0.03457269\n",
            "Iteration 182, loss = 0.03417103\n",
            "Iteration 183, loss = 0.03391780\n",
            "Iteration 184, loss = 0.03375481\n",
            "Iteration 185, loss = 0.03358173\n",
            "Iteration 186, loss = 0.03347685\n",
            "Iteration 187, loss = 0.03348010\n",
            "Iteration 188, loss = 0.03354823\n",
            "Iteration 189, loss = 0.03369215\n",
            "Iteration 190, loss = 0.03429620\n",
            "Iteration 191, loss = 0.03484690\n",
            "Iteration 192, loss = 0.03639860\n",
            "Iteration 193, loss = 0.03566597\n",
            "Iteration 194, loss = 0.03540946\n",
            "Iteration 195, loss = 0.03312747\n",
            "Iteration 196, loss = 0.03201108\n",
            "Iteration 197, loss = 0.03213014\n",
            "Iteration 198, loss = 0.03294773\n",
            "Iteration 199, loss = 0.03420509\n",
            "Iteration 200, loss = 0.03345519\n",
            "Iteration 201, loss = 0.03254009\n",
            "Iteration 202, loss = 0.03136282\n",
            "Iteration 203, loss = 0.03112372\n",
            "Iteration 204, loss = 0.03159591\n",
            "Iteration 205, loss = 0.03200273\n",
            "Iteration 206, loss = 0.03226798\n",
            "Iteration 207, loss = 0.03152416\n",
            "Iteration 208, loss = 0.03088193\n",
            "Iteration 209, loss = 0.03037596\n",
            "Iteration 210, loss = 0.03031960\n",
            "Iteration 211, loss = 0.03052047\n",
            "Iteration 212, loss = 0.03068102\n",
            "Iteration 213, loss = 0.03081520\n",
            "Iteration 214, loss = 0.03055655\n",
            "Iteration 215, loss = 0.03032953\n",
            "Iteration 216, loss = 0.02991405\n",
            "Iteration 217, loss = 0.02952803\n",
            "Iteration 218, loss = 0.02932055\n",
            "Iteration 219, loss = 0.02915803\n",
            "Iteration 220, loss = 0.02910403\n",
            "Iteration 221, loss = 0.02906710\n",
            "Iteration 222, loss = 0.02914795\n",
            "Iteration 223, loss = 0.02927274\n",
            "Iteration 224, loss = 0.02961244\n",
            "Iteration 225, loss = 0.02984100\n",
            "Iteration 226, loss = 0.03069248\n",
            "Iteration 227, loss = 0.03076325\n",
            "Iteration 228, loss = 0.03183562\n",
            "Iteration 229, loss = 0.03070603\n",
            "Iteration 230, loss = 0.03029360\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Test accuracy:  0.95\n",
            "Train accuracy:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sdM_pY_0d5cm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}