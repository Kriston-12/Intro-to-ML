{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'derivativeOutput' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 250\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion Matrix is from Part 1a is: \u001b[39m\u001b[38;5;124m\"\u001b[39m,cM)\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion Matrix from Part 1b is:\u001b[39m\u001b[38;5;124m\"\u001b[39m,sciKit)\n\u001b[1;32m--> 250\u001b[0m \u001b[43mtest_Part1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 239\u001b[0m, in \u001b[0;36mtest_Part1\u001b[1;34m()\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m         y_test[j]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 239\u001b[0m err,w\u001b[38;5;241m=\u001b[39m\u001b[43mfit_NeuralNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m plotErr(err,\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m    243\u001b[0m cM\u001b[38;5;241m=\u001b[39mconfMatrix(X_test,y_test,w)\n",
      "Cell \u001b[1;32mIn[1], line 55\u001b[0m, in \u001b[0;36mfit_NeuralNetwork\u001b[1;34m(X_train, y_train, alpha, hidden_layer_sizes, epochs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m#TODO: Model Update: Forward Propagation, Backpropagation\u001b[39;00m\n\u001b[0;32m     54\u001b[0m X, S \u001b[38;5;241m=\u001b[39m forwardPropagation(x, weights)\n\u001b[1;32m---> 55\u001b[0m gradList \u001b[38;5;241m=\u001b[39m \u001b[43mbackPropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# update the weight and calculate the error\u001b[39;00m\n\u001b[0;32m     58\u001b[0m weights \u001b[38;5;241m=\u001b[39m updateWeights(weights ,gradList, alpha)\n",
      "Cell \u001b[1;32mIn[1], line 121\u001b[0m, in \u001b[0;36mbackPropagation\u001b[1;34m(X, y_n, s, weights)\u001b[0m\n\u001b[0;32m    114\u001b[0m delL\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# To be able to complete this function, you need to understand this line below\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# In this line, we are computing the derivative of the Loss function w.r.t the \u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# output layer (without activation). This is dL/dS[l-2]\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# By chain rule, dL/dS[l-2] = dL/dy * dy/dS[l-2] . Now dL/dy is the derivative Error and \u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# dy/dS[l-2]  is the derivative output.\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m delL\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m,derivativeError(X[l\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],y_n)\u001b[38;5;241m*\u001b[39m\u001b[43mderivativeOutput\u001b[49m(s[l\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m])) \u001b[38;5;66;03m# We use s[l - 2] is bc X[i] is always after activation of previous s[i - 1]\u001b[39;00m\n\u001b[0;32m    122\u001b[0m curr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Now, let's calculate dL/dS[l-2], dL/dS[l-3],...\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'derivativeOutput' is not defined"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"NN\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1-1ybgpW2N3QLFS9FHdWIo3VCR7VE0lLW\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def fit_NeuralNetwork(X_train,y_train,alpha,hidden_layer_sizes,epochs):\n",
    "    # Initialize the epoch errors\n",
    "    err=np.zeros((epochs,1))\n",
    "    \n",
    "    # Initialize the architecture\n",
    "    N, d = X_train.shape\n",
    "    X0 = np.ones((N,1))\n",
    "    X_train = np.hstack((X0,X_train))\n",
    "    d = d + 1\n",
    "    L = len(hidden_layer_sizes)\n",
    "    L = L + 2 # L is the total layer = hidden layer + 2\n",
    "    \n",
    "    #Initializing the weights for input layer, separately initialize this one\n",
    "    weight_layer = np.random.normal(0, 0.1, (d,hidden_layer_sizes[0])) #np.ones((d,hidden_layer_sizes[0]))\n",
    "    weights = []\n",
    "    weights.append(weight_layer) #append(0.1*weight_layer)\n",
    "    \n",
    "    #Initializing the weights for hidden layers\n",
    "    for l in range(L-3):\n",
    "        # first hidden[l] + 1 means we account for bias, \n",
    "        weight_layer = np.random.normal(0, 0.1, (hidden_layer_sizes[l]+1,hidden_layer_sizes[l+1])) \n",
    "        weights.append(weight_layer) \n",
    "\n",
    "    #Initializing the weights for output layers\n",
    "    weight_layer= np.random.normal(0, 0.1, (hidden_layer_sizes[l+1]+1,1)) \n",
    "    weights.append(weight_layer) \n",
    "    \n",
    "    for e in range(epochs):\n",
    "        choiceArray=np.arange(0, N)\n",
    "        np.random.shuffle(choiceArray)\n",
    "        errN=0\n",
    "        for n in range(N):\n",
    "            index=choiceArray[n]\n",
    "            x=np.transpose(X_train[index])\n",
    "            #TODO: Model Update: Forward Propagation, Backpropagation\n",
    "            X, S = forwardPropagation(x, weights)\n",
    "            gradList = backPropagation(X, y_train[index], S, weights)\n",
    "\n",
    "            # update the weight and calculate the error\n",
    "            weights = updateWeights(weights ,gradList, alpha)\n",
    "            errN += errorPerSample(X, y_train[index])\n",
    "        err[e]=errN/N \n",
    "    return err, weights\n",
    "\n",
    "def activation(s):\n",
    "    return s if s > 0 else 0\n",
    "\n",
    "def derivativeActivation(s):\n",
    "    return 1 if s > 0 else 0\n",
    "\n",
    "def derivativeError(x_L,y):\n",
    "    if y == 1:\n",
    "        return -1 / x_L\n",
    "    elif y == -1:\n",
    "        return 1 / (1 - x_L)\n",
    "    \n",
    "\n",
    "def outputf(s):\n",
    "    ret = 1 / (1 + np.exp(-s))\n",
    "    return ret\n",
    "\n",
    "def errorf(x_L,y):\n",
    "    if y == 1:\n",
    "        return -1 * np.log(x_L)\n",
    "    elif y == -1:\n",
    "        return -1 * np.log(1 - x_L)\n",
    "\n",
    "\n",
    "def forwardPropagation(x, weights):\n",
    "    l=len(weights)+1 # l now is layer number \n",
    "    currX = x\n",
    "    retS=[]\n",
    "    retX=[]\n",
    "    retX.append(currX)\n",
    "\n",
    "    for i in range(l-1): # we only loop l - 1 layer since the last layer \n",
    "        \n",
    "        currS= np.matmul(np.transpose(weights[i]), currX) # weights[i].shape = (a, b), curX.shape = a, c\n",
    "        retS.append(currS)\n",
    "        currX=currS\n",
    "        if i != len(weights)-1:\n",
    "            for j in range(len(currS)):\n",
    "                currX[j] = activation(currX[j])\n",
    "            currX = np.hstack((1,currX))\n",
    "        else:\n",
    "            currX = outputf(currX)\n",
    "        retX.append(currX)\n",
    "    return retX,retS\n",
    "\n",
    "\n",
    "def backPropagation(X,y_n,s,weights): \n",
    "    #x:0,1,...,L\n",
    "    #S:1,...,L\n",
    "    #weights: 1,...,L\n",
    "    l=len(X)\n",
    "    delL=[]\n",
    "\n",
    "    # To be able to complete this function, you need to understand this line below\n",
    "    # In this line, we are computing the derivative of the Loss function w.r.t the \n",
    "    # output layer (without activation). This is dL/dS[l-2]\n",
    "    # By chain rule, dL/dS[l-2] = dL/dy * dy/dS[l-2] . Now dL/dy is the derivative Error and \n",
    "    # dy/dS[l-2]  is the derivative output.\n",
    "    delL.insert(0,derivativeError(X[l-1],y_n)*derivativeOutput(s[l-2])) # We use s[l - 2] is bc X[i] is always after activation of previous s[i - 1]\n",
    "    curr=0\n",
    "    \n",
    "    # Now, let's calculate dL/dS[l-2], dL/dS[l-3],...\n",
    "    for i in range(len(X)-2, 0, -1): #L-1,...,0\n",
    "        delNextLayer=delL[curr]\n",
    "        WeightsNextLayer=weights[i]\n",
    "        sCur=s[i-1]\n",
    "        \n",
    "        #Init this to 0s vector\n",
    "        delN=np.zeros((len(s[i-1]),1))\n",
    "\n",
    "        #Now we calculate the gradient backward\n",
    "        #Remember: dL/dS[i] = dL/dS[i+1] * W(which W???) * activation\n",
    "        for j in range(len(s[i-1])): #number of nodes in layer i - 1\n",
    "            for k in range(len(s[i])): #number of nodes in layer i \n",
    "              \n",
    "                delN[j] = delN[j] + delNextLayer[k] * WeightsNextLayer[j + 1][k]\n",
    "            delN[j] = delN[j] * derivativeActivation(sCur[j])\n",
    "        delL.insert(0,delN)\n",
    "    \n",
    "    # We have all the deltas we need. Now, we need to find dL/dW.\n",
    "    # It's very simple now, dL/dW = dL/dS * dS/dW = dL/dS * X\n",
    "    g=[]\n",
    "    for i in range(len(delL)):\n",
    "        rows,cols=weights[i].shape\n",
    "        gL=np.zeros((rows,cols))\n",
    "        currX=X[i]\n",
    "        currdelL=delL[i]\n",
    "        for j in range(rows):\n",
    "            for k in range(cols):\n",
    "                #TODO: Calculate the gradient using currX and currdelL\n",
    "                gL[j,k] = currX[j].item() * currdelL[k].item() # I think we could just do currdelL[k] * currX[j]\n",
    "        g.append(gL)\n",
    "    return g\n",
    "\n",
    "def updateWeights(weights,g,alpha):\n",
    "    nW=[]\n",
    "    for i in range(len(weights)):\n",
    "        rows, cols = weights[i].shape\n",
    "        currWeight=weights[i]\n",
    "        currG=g[i]\n",
    "        for j in range(rows):\n",
    "            for k in range(cols):\n",
    "                #Gradient Descent Update\n",
    "                currWeight[j,k] = currWeight[j,k] - alpha * currG[j,k]\n",
    "        nW.append(currWeight)\n",
    "    return nW\n",
    "\n",
    "\n",
    "def errorPerSample(X,y_n):\n",
    "    return errorf(X[len(X) - 1], y_n)\n",
    "\n",
    "\n",
    "def pred(x_n,weights):\n",
    "    # TODO: prediction using the forwardPropagation function\n",
    "    retX,retS= forwardPropagation(x_n,weights)\n",
    "    l=len(retX)\n",
    "\n",
    "    # Return -1 if probability lesser than 0.5\n",
    "    # Else return 1\n",
    "    if retX[l-1]<0.5:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def confMatrix(X_train,y_train,w):\n",
    "    eCount=np.zeros((2,2), dtype=np.int8)\n",
    "    row, col = X_train.shape\n",
    "    X0 = np.ones((row,1))\n",
    "    X_train = np.hstack((X0,X_train))\n",
    "    for j in range(row):\n",
    "        if (pred(X_train[j],w) == -1 and y_train[j] == -1):\n",
    "            eCount[0,0] += 1\n",
    "        elif (pred(X_train[j],w) == 1 and y_train[j] == -1): \n",
    "            eCount[0,1] += 1\n",
    "        elif (pred(X_train[j],w) == 1 and y_train[j] == 1):\n",
    "            eCount[1,1] += 1\n",
    "        else:\n",
    "            eCount[1,0] += 1\n",
    "    return eCount\n",
    "\n",
    "def plotErr(e,epochs):\n",
    "    x = np.arange(1, epochs + 1)\n",
    "    y = e\n",
    "    plt.title(\"Error vs. Epoch\") \n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.plot(x,y,linewidth=2.0)\n",
    "    plt.show()\n",
    "\n",
    "def test_SciKit(X_train, X_test, Y_train, Y_test):\n",
    "    NN = MLPClassifier(solver='adam', alpha = 0.00001, \n",
    "                       hidden_layer_sizes=(30, 10), random_state = 1)\n",
    "    \n",
    "    NN.fit(X_train, Y_train)\n",
    "\n",
    "    y_pred = NN.predict(X_test)\n",
    "\n",
    "    cM = confusion_matrix(Y_test, y_pred)\n",
    "    return cM\n",
    "\n",
    "def test_Part1():\n",
    "    from sklearn.datasets import load_iris\n",
    "    X_train, y_train = load_iris(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train[50:],y_train[50:],test_size=0.2, random_state=1)\n",
    "    \n",
    "    for i in range(80):\n",
    "        if y_train[i]==1:\n",
    "            y_train[i]=-1\n",
    "        else:\n",
    "            y_train[i]=1\n",
    "    for j in range(20):\n",
    "        if y_test[j]==1:\n",
    "            y_test[j]=-1\n",
    "        else:\n",
    "            y_test[j]=1\n",
    "        \n",
    "    err,w=fit_NeuralNetwork(X_train,y_train,1e-2,[30, 10],100)\n",
    "    \n",
    "    plotErr(err,100)\n",
    "    \n",
    "    cM=confMatrix(X_test,y_test,w)\n",
    "    \n",
    "    sciKit=test_SciKit(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    print(\"Confusion Matrix is from Part 1a is: \",cM)\n",
    "    print(\"Confusion Matrix from Part 1b is:\",sciKit)\n",
    "\n",
    "test_Part1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
